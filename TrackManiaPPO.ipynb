{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tmrl\n",
    "import time\n",
    "import matplotlib.pyplot as plts\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4090'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space:\t Tuple(Box(0.0, 1000.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Box(0.0, inf, (4, 19), float32), Box(-1.0, 1.0, (3,), float32), Box(-1.0, 1.0, (3,), float32))\n",
      "Action Space:\t\t Box(-1.0, 1.0, (3,), float32)\n",
      "observation_space logits: 84\n",
      "action_space logits:\t 3\n"
     ]
    }
   ],
   "source": [
    "env = tmrl.get_environment()\n",
    "print('Observation Space:\\t', env.observation_space)\n",
    "print('Action Space:\\t\\t', env.action_space)\n",
    "observation_space = np.sum([np.product(value.shape) for value in env.observation_space])\n",
    "action_space = env.action_space.shape[0]\n",
    "print('observation_space logits:', observation_space)\n",
    "print('action_space logits:\\t', action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {'policy_lr': 1e-5,\n",
    "                'critic_lr': 1e-5,\n",
    "                'gamma': 0.996,\n",
    "                'clip_coef': 0.2,\n",
    "                'critic_coef': 0.1,\n",
    "                'entropy_coef': 0.1,\n",
    "                'batch_size': 256,\n",
    "                'num_updates': 10000,\n",
    "                'epochs_per_update': 100,\n",
    "                'hidden_dim':512,\n",
    "                'max_episode_steps': 2400,\n",
    "                'norm_advantages': True,\n",
    "                'grad_clip_val': 0.1,\n",
    "                'initial_std': 1,\n",
    "                'avg_ray': 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_mean = nn.Sequential(\n",
    "            #nn.LayerNorm(observation_space),\n",
    "            #nn.BatchNorm1d(observation_space),\n",
    "            nn.Linear(observation_space, hyper_params['hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyper_params['hidden_dim'],hyper_params['hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyper_params['hidden_dim'], action_space),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.actor_logvar = nn.Sequential(\n",
    "            #nn.LayerNorm(observation_space),\n",
    "            #nn.BatchNorm1d(observation_space),\n",
    "            nn.Linear(observation_space, hyper_params['hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyper_params['hidden_dim'],hyper_params['hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyper_params['hidden_dim'], 1)\n",
    "        )\n",
    "\n",
    "    def sample_action_with_logprobs(self, observation):\n",
    "        dist = self(observation)\n",
    "        sample_action = dist.sample()\n",
    "        return sample_action, dist.log_prob(sample_action)\n",
    "    \n",
    "    def mean_only(self, observation):\n",
    "        with torch.no_grad():\n",
    "            return self.action_mean(observation)\n",
    "    \n",
    "    def get_action_log_prob(self, observation, action):\n",
    "        dist = self(observation)\n",
    "        return dist.log_prob(action)\n",
    "    \n",
    "    def forward(self, observation):\n",
    "        observation /= hyper_params['avg_ray']\n",
    "        means = self.action_mean(observation)\n",
    "        vars = torch.zeros(observation.shape[0], action_space).to(device)\n",
    "        vars[:,:] = self.actor_logvar(observation).exp().view(-1,1)\n",
    "        covar_mat = torch.zeros(observation.shape[0], action_space, action_space).to(device)\n",
    "        covar_mat[:,np.arange(action_space), np.arange(action_space)] = vars\n",
    "\n",
    "        dist = torch.distributions.MultivariateNormal(means, covar_mat)\n",
    "        return dist\n",
    "        \n",
    "class Critic (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            #nn.LayerNorm(observation_space),\n",
    "            #nn.BatchNorm1d(observation_space),\n",
    "            nn.Linear(observation_space, hyper_params['hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyper_params['hidden_dim'],hyper_params['hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyper_params['hidden_dim'], 1)\n",
    "        )\n",
    "    def forward(self, observation):\n",
    "        observation /= hyper_params['avg_ray']\n",
    "        return self.network(observation)\n",
    "    \n",
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.policy = Policy()\n",
    "        self.critic = Critic()\n",
    "    def forward(self, x):\n",
    "        raise SyntaxError('Propagate through Agent.policy \\\n",
    "                          and Agent.critic individually')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_obs_to_tensor(observations):\n",
    "    tensors = [torch.tensor(observation).view(-1) for observation in observations]\n",
    "    return torch.cat(tuple(tensors), dim=-1)\n",
    "\n",
    "def env_act_to_tensor(action):\n",
    "    return torch.tensor(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent().to(device)\n",
    "policy_optim = torch.optim.Adam(agent.policy.parameters(), lr=hyper_params['policy_lr'])\n",
    "critic_optim = torch.optim.Adam(agent.critic.parameters(), lr=hyper_params['critic_lr'])\n",
    "#agent.load_state_dict(torch.load('130.33999633789062RewardRacer56Update.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PPO():\n",
    "    cum_rewards = []\n",
    "    actor_losses = []\n",
    "    critic_losses = []\n",
    "    total_losses = []\n",
    "\n",
    "    cum_reward = 0\n",
    "    for update in range(hyper_params['num_updates']):        \n",
    "        ##info to record per episode\n",
    "        obs = torch.zeros(hyper_params['max_episode_steps'], observation_space)\n",
    "        actions = torch.zeros(hyper_params['max_episode_steps'], action_space)\n",
    "        logprobs = torch.zeros(hyper_params['max_episode_steps'])\n",
    "        rewards = torch.zeros(hyper_params['max_episode_steps'])\n",
    "        state_values = torch.zeros(hyper_params['max_episode_steps'])\n",
    "        returns = torch.zeros(hyper_params['max_episode_steps'])\n",
    "        \n",
    "        #check if I should use a learning rate scheduler ##\n",
    "        \n",
    "        #reset  the enviornment before each new episode\n",
    "        next_obs = env_obs_to_tensor(env.reset()[0]) #just grab obs\n",
    "        \n",
    "        ##this is according to tmrl docs - \n",
    "        ##have to manually click the game to ensure focus on\n",
    "        ##window when training \n",
    "        #if update == 0:\n",
    "            #time.sleep(1.0)\n",
    "        \n",
    "        max_idx = 0\n",
    "        was_terminated = False\n",
    "        agent.eval()\n",
    "        for step in range(hyper_params['max_episode_steps']):\n",
    "            obs[step] = next_obs\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action, logprob = agent.policy.sample_action_with_logprobs(next_obs.to(device).unsqueeze(0))\n",
    "                state_value = agent.critic(next_obs.to(device).unsqueeze(0))\n",
    "            actions[step] = action[0]\n",
    "            logprobs[step] = logprob[0]\n",
    "            state_values[step] = state_value[0]\n",
    "\n",
    "            ##actions are sampled from gaussian distribution - could be greater\n",
    "            ##in maginitude than the action space allowed range\n",
    "            clamped_action = np.clip(np.array(action.cpu()),-1,1)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, info = env.step(clamped_action[0])\n",
    "\n",
    "            #terminate episode if stuck on a rail\n",
    "            if next_obs[2][next_obs[2] <= 40].sum() > 0:\n",
    "                terminated = True\n",
    "            \n",
    "            #print('reward here', reward)\n",
    "            rewards[step] = torch.tensor(reward)\n",
    "            next_obs = env_obs_to_tensor(next_obs)\n",
    "            if terminated or truncated:\n",
    "                was_terminated = True\n",
    "                max_idx = step\n",
    "                break\n",
    "        ##pause environment according to tmrl\n",
    "        env.wait()\n",
    "        ##\n",
    "        max_idx = step\n",
    "        ##calculate cumulative rewards - state values\n",
    "        with torch.no_grad():\n",
    "            for t in range(max_idx + 1)[::-1]:\n",
    "                if t == (max_idx):\n",
    "                    if not was_terminated:\n",
    "                        ##bootstrap value with critic estimation if episode wasn't terminated but went max steps\n",
    "                        returns[t] = rewards[t] #+ (hyper_params['gamma']*agent.critic(next_obs.to(device)))\n",
    "                    else:\n",
    "                        returns[t] = rewards[t]\n",
    "                else:\n",
    "                    returns[t] = rewards[t] + (hyper_params['gamma']*returns[t+1])\n",
    "                    #print('here', returns[t], 't=', t)\n",
    "            advantages = returns - state_values\n",
    "            cum_reward = rewards.sum().item()\n",
    "        #print('Update', update + 1, 'Cumulative rewrards:', returns[0])\n",
    "        rand_idxs = np.random.permutation(np.arange(max_idx+1))\n",
    "        epochs_values_loss = []\n",
    "        epochs_ppo_loss = []\n",
    "        epochs_total_loss = []\n",
    "        \n",
    "        if cum_reward > 200:\n",
    "            torch.save(agent.state_dict(), f'Y{cum_reward:.2f}RewardRacer{update}Update_2.pt')\n",
    "        \n",
    "        agent.train()\n",
    "        for epoch in range(hyper_params['epochs_per_update']):\n",
    "            for batch_start_idx in range(0, max_idx, hyper_params['batch_size']):\n",
    "                batch_end_idx = batch_start_idx + hyper_params['batch_size']\n",
    "                batch_idxs = rand_idxs[batch_start_idx:batch_end_idx]\n",
    "\n",
    "                batch_obs = obs[batch_idxs]\n",
    "                batch_actions = actions[batch_idxs]\n",
    "\n",
    "                ##calculate the ppo objective\n",
    "                #############################################################################################################################\n",
    "                batch_new_log_probs = agent.policy.get_action_log_prob(batch_obs.to(device), batch_actions.to(device))\n",
    "                batch_old_log_probs = logprobs[batch_idxs].to(device)\n",
    "\n",
    "                log_ratio = batch_new_log_probs - batch_old_log_probs\n",
    "                ratio = log_ratio.exp()\n",
    "\n",
    "                ##calculate the surrogate objective function\n",
    "                batch_advantages = advantages[batch_idxs].to(device)\n",
    "                if hyper_params['norm_advantages']:\n",
    "                    batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-8)\n",
    "\n",
    "                unclipped_obj = -ratio * batch_advantages\n",
    "                clipped_obj = -torch.clip(ratio, 1 - hyper_params['clip_coef'], 1 + hyper_params['clip_coef']) * batch_advantages\n",
    "                ppo_loss = torch.max(unclipped_obj, clipped_obj).sum() / hyper_params['batch_size'] ##not .mean() in case of small last batch\n",
    "                epochs_ppo_loss.append(ppo_loss.item())\n",
    "\n",
    "                ##value loss is literally just mse loss of actual on-polcy cumulative rewards\n",
    "                ##we already made the advantage - we only train our critic here\n",
    "                new_state_values = agent.critic(batch_obs.to(device))\n",
    "                v_loss = ((new_state_values.view(-1) - returns[batch_idxs].to(device))**2).sum() / hyper_params['batch_size'] ##not .mean() in case of small last batch\n",
    "                epochs_values_loss.append(v_loss.item())\n",
    "            \n",
    "                total_loss = ppo_loss + hyper_params['critic_coef']*v_loss\n",
    "                epochs_total_loss.append(total_loss.item())\n",
    "                \n",
    "                policy_optim.zero_grad()\n",
    "                critic_optim.zero_grad()\n",
    "                \n",
    "                total_loss.backward()\n",
    "\n",
    "                nn.utils.clip_grad.clip_grad_value_(agent.policy.parameters(), clip_value=hyper_params['grad_clip_val'])\n",
    "                for param in agent.policy.parameters():\n",
    "                    mask = torch.isnan(param.grad)\n",
    "                    param.grad[mask] = 0.0\n",
    "                    if mask.sum == param.numel():\n",
    "                        print('code is broken, yup')\n",
    "                policy_optim.step()\n",
    "\n",
    "                nn.utils.clip_grad.clip_grad_value_(agent.critic.parameters(), clip_value=hyper_params['grad_clip_val'])\n",
    "                critic_optim.step()\n",
    "        print('Update', update + 1)\n",
    "        print('actor loss', np.mean(epochs_ppo_loss))\n",
    "        print('critic loss', np.mean(epochs_values_loss))\n",
    "        print('total loss', np.mean(epochs_total_loss))\n",
    "        print('total reward', cum_reward)\n",
    "        cum_rewards.append(cum_reward)\n",
    "        actor_losses.append(np.mean(epochs_ppo_loss))\n",
    "        critic_losses.append(np.mean(epochs_values_loss))\n",
    "        total_losses.append(np.mean(epochs_total_loss))\n",
    "    return cum_rewards, actor_losses, critic_losses, total_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 21.87463000006028\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([0.], dtype=float32),\n",
       "  array([0.], dtype=float32),\n",
       "  array([[457., 465., 410., 305., 247., 212., 191., 197., 174., 171., 173.,\n",
       "          182., 191., 212., 247., 304., 413., 465., 458.],\n",
       "         [457., 465., 410., 305., 247., 212., 191., 197., 174., 171., 173.,\n",
       "          182., 191., 212., 247., 304., 413., 465., 458.],\n",
       "         [457., 465., 410., 305., 247., 212., 191., 197., 174., 171., 173.,\n",
       "          182., 191., 212., 247., 304., 413., 465., 458.],\n",
       "         [457., 465., 410., 305., 247., 212., 191., 197., 174., 171., 173.,\n",
       "          182., 191., 212., 247., 304., 413., 465., 458.]], dtype=float32),\n",
       "  array([0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0.], dtype=float32)),\n",
       " {})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(1)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 3.876891000001706\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 1\n",
      "actor loss -0.009400872273836285\n",
      "critic loss 2.023207831978798\n",
      "total loss 0.19291991442441941\n",
      "total reward 5.279999732971191\n",
      "Update 2\n",
      "actor loss -0.010008172020316125\n",
      "critic loss 0.9862292709946633\n",
      "total loss 0.08861475693061947\n",
      "total reward 7.090000152587891\n",
      "Update 3\n",
      "actor loss -0.01111858032643795\n",
      "critic loss 0.2712934844195843\n",
      "total loss 0.016010768599808215\n",
      "total reward 3.2699999809265137\n",
      "Update 4\n",
      "actor loss -0.009975759582594036\n",
      "critic loss 0.30344767570495607\n",
      "total loss 0.020369008490815757\n",
      "total reward 5.539999961853027\n",
      "Update 5\n",
      "actor loss -0.017283723820000887\n",
      "critic loss 0.41214772647246717\n",
      "total loss 0.023931049574166537\n",
      "total reward 0.5199999809265137\n",
      "Update 6\n",
      "actor loss -0.00830787731334567\n",
      "critic loss 0.02597520312294364\n",
      "total loss -0.0057103569549508395\n",
      "total reward 0.7399999499320984\n",
      "Update 7\n",
      "actor loss -0.0051650963537395005\n",
      "critic loss 0.025153762307018043\n",
      "total loss -0.0026497200899757447\n",
      "total reward 0.6799999475479126\n",
      "Update 8\n",
      "actor loss -0.003884077128022909\n",
      "critic loss 0.6005837664256494\n",
      "total loss 0.056174300890415904\n",
      "total reward 6.079999923706055\n",
      "Update 9\n",
      "actor loss -0.014516228577122092\n",
      "critic loss 0.339207331687212\n",
      "total loss 0.01940450499765575\n",
      "total reward 1.2899998426437378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 0.3386043001082726\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 10\n",
      "actor loss -0.024083445649594068\n",
      "critic loss 3.761578140258789\n",
      "total loss 0.3520743757486343\n",
      "total reward 7.210000514984131\n",
      "Update 11\n",
      "actor loss -0.006635284470394254\n",
      "critic loss 1.2799148745834827\n",
      "total loss 0.12135620509274304\n",
      "total reward 0.47999995946884155\n",
      "Update 12\n",
      "actor loss -0.019263005927205087\n",
      "critic loss 0.35521884948015214\n",
      "total loss 0.016258879620581864\n",
      "total reward 3.2099997997283936\n",
      "Update 13\n",
      "actor loss -0.01254948424641043\n",
      "critic loss 0.15089648276567458\n",
      "total loss 0.0025401642266660927\n",
      "total reward 3.4100000858306885\n",
      "Update 14\n",
      "actor loss -0.009174697633522253\n",
      "critic loss 1.0001399705807368\n",
      "total loss 0.09083930103729168\n",
      "total reward 9.109999656677246\n",
      "Update 15\n",
      "actor loss -0.024157336093485356\n",
      "critic loss 2.4907375955581665\n",
      "total loss 0.22491642817854882\n",
      "total reward 6.369999408721924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 0.05194030024722451\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 16\n",
      "actor loss -0.011489919777959585\n",
      "critic loss 2.442085521221161\n",
      "total loss 0.23271863639354706\n",
      "total reward 10.5\n",
      "Update 17\n",
      "actor loss -0.009684189958497881\n",
      "critic loss 1.1443269261717797\n",
      "total loss 0.10474850432947277\n",
      "total reward 9.74000072479248\n",
      "Update 18\n",
      "actor loss -0.006086992784403265\n",
      "critic loss 1.096986515522003\n",
      "total loss 0.10361166054848582\n",
      "total reward 10.069999694824219\n",
      "Update 19\n",
      "actor loss -0.011931394655257464\n",
      "critic loss 1.1214669926464558\n",
      "total loss 0.10021530678495764\n",
      "total reward 9.739999771118164\n",
      "Update 20\n",
      "actor loss -0.004262599921785295\n",
      "critic loss 1.1946022525429725\n",
      "total loss 0.11519762700423598\n",
      "total reward 9.760000228881836\n",
      "Update 21\n",
      "actor loss -0.007044869031524285\n",
      "critic loss 1.0002464285492898\n",
      "total loss 0.09297977544600144\n",
      "total reward 9.90999984741211\n",
      "Update 22\n",
      "actor loss -0.011514572785235941\n",
      "critic loss 0.732220496237278\n",
      "total loss 0.061707477830350396\n",
      "total reward 9.039999008178711\n",
      "Update 23\n",
      "actor loss -0.023350348994135857\n",
      "critic loss 0.4023406708240509\n",
      "total loss 0.016883718743920327\n",
      "total reward 4.419999599456787\n",
      "Update 24\n",
      "actor loss -0.007062834370881319\n",
      "critic loss 2.378155977129936\n",
      "total loss 0.2307527669146657\n",
      "total reward 11.730000495910645\n",
      "Update 25\n",
      "actor loss -0.012159576062113047\n",
      "critic loss 1.2240412205457687\n",
      "total loss 0.11024454861879349\n",
      "total reward 9.519999504089355\n",
      "Update 26\n",
      "actor loss -0.007888762876391411\n",
      "critic loss 0.8434282100852579\n",
      "total loss 0.07645405944320373\n",
      "total reward 8.90999984741211\n",
      "Update 27\n",
      "actor loss -0.015174203105270862\n",
      "critic loss 1.7630368304252624\n",
      "total loss 0.16112948298454285\n",
      "total reward 8.100000381469727\n",
      "Update 28\n",
      "actor loss -0.017368086646310985\n",
      "critic loss 1.2437258434295655\n",
      "total loss 0.10700449984520674\n",
      "total reward 10.929999351501465\n",
      "Update 29\n",
      "actor loss -0.0066225876100361345\n",
      "critic loss 0.5527076874673367\n",
      "total loss 0.048648182237520814\n",
      "total reward 10.199999809265137\n",
      "Update 30\n",
      "actor loss -0.005197162416297942\n",
      "critic loss 0.8166461768746376\n",
      "total loss 0.07646745739504696\n",
      "total reward 9.09999942779541\n",
      "Update 31\n",
      "actor loss -0.00826502291020006\n",
      "critic loss 0.5379905610159039\n",
      "total loss 0.04553403402562253\n",
      "total reward 5.779999732971191\n",
      "Update 32\n",
      "actor loss -0.02067370317876339\n",
      "critic loss 5.754323809146881\n",
      "total loss 0.554758687466383\n",
      "total reward 15.869999885559082\n",
      "Update 33\n",
      "actor loss -0.012529822373762727\n",
      "critic loss 1.5003849840164185\n",
      "total loss 0.13750867925584317\n",
      "total reward 11.200000762939453\n",
      "Update 34\n",
      "actor loss -0.00548166761174798\n",
      "critic loss 1.1949741958081723\n",
      "total loss 0.11401575423777104\n",
      "total reward 10.179999351501465\n",
      "Update 35\n",
      "actor loss -0.007663856572471559\n",
      "critic loss 1.020554626584053\n",
      "total loss 0.09439160812646151\n",
      "total reward 9.589999198913574\n",
      "Update 36\n",
      "actor loss -0.009517920885700732\n",
      "critic loss 0.7835969139635562\n",
      "total loss 0.06884177210740745\n",
      "total reward 9.970000267028809\n",
      "Update 37\n",
      "actor loss -0.01081782088149339\n",
      "critic loss 1.241582762748003\n",
      "total loss 0.11334045697003603\n",
      "total reward 9.710000038146973\n",
      "Update 38\n",
      "actor loss -0.009881193963810802\n",
      "critic loss 1.2073146797716618\n",
      "total loss 0.11085027600638568\n",
      "total reward 10.1899995803833\n",
      "Update 39\n",
      "actor loss -0.006182302106171846\n",
      "critic loss 1.3760283660888672\n",
      "total loss 0.13142053812742233\n",
      "total reward 9.980000495910645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12638 (__send_act_get_obs_and_wait):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py\", line 439, in __send_act_get_obs_and_wait\n",
      "    self.__update_obs_rew_terminated_truncated()  # capture observation\n",
      "  File \"C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py\", line 453, in __update_obs_rew_terminated_truncated\n",
      "    o, r, d, i = self.interface.get_obs_rew_terminated_info()\n",
      "  File \"C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tmrl\\custom\\custom_gym_interfaces.py\", line 337, in get_obs_rew_terminated_info\n",
      "    img, speed, data = self.grab_lidar_speed_and_data()\n",
      "  File \"C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tmrl\\custom\\custom_gym_interfaces.py\", line 259, in grab_lidar_speed_and_data\n",
      "    data = self.client.retrieve_data()\n",
      "  File \"C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tmrl\\custom\\utils\\tools.py\", line 72, in retrieve_data\n",
      "    assert t_now - t_start < timeout, f\"OpenPlanet stopped sending data since more than {timeout}s.\"\n",
      "AssertionError: OpenPlanet stopped sending data since more than 10.0s.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cum_rewards, actor_losses, critic_losses, total_losses \u001b[39m=\u001b[39m train_PPO()\n",
      "Cell \u001b[1;32mIn[19], line 45\u001b[0m, in \u001b[0;36mtrain_PPO\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39m##actions are sampled from gaussian distribution - could be greater\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m##in maginitude than the action space allowed range\u001b[39;00m\n\u001b[0;32m     43\u001b[0m clamped_action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(np\u001b[39m.\u001b[39marray(action\u001b[39m.\u001b[39mcpu()),\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m next_obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clamped_action[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     47\u001b[0m \u001b[39m#terminate episode if stuck on a rail\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m next_obs[\u001b[39m2\u001b[39m][next_obs[\u001b[39m2\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m40\u001b[39m]\u001b[39m.\u001b[39msum() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\core.py:408\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\n\u001b[0;32m    405\u001b[0m     \u001b[39mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    406\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[0;32m    407\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tmrl\\wrappers.py:33\u001b[0m, in \u001b[0;36mFloat64ToFloat32.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m---> 33\u001b[0m     s, r, d, t, info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m s, r, d, t, info\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\core.py:469\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\n\u001b[0;32m    466\u001b[0m     \u001b[39mself\u001b[39m, action: ActType\n\u001b[0;32m    467\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[0;32m    468\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:545\u001b[0m, in \u001b[0;36mRealTimeEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbenchmark:\n\u001b[0;32m    544\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbench\u001b[39m.\u001b[39mstart_step_time()\n\u001b[1;32m--> 545\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_join_thread()\n\u001b[0;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    547\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_buf\u001b[39m.\u001b[39mappend(action)  \u001b[39m# the action is always appended to the buffer\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:389\u001b[0m, in \u001b[0;36mRealTimeEnv._join_thread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"This is called at the beginning of every user-side API functions (step(), reset()...) for thread safety.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[39mThis ensures that the previous time-step is completed when starting a new one\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39masync_threading:\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_at_thread\u001b[39m.\u001b[39;49mjoin()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[0;32m   1097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1116\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[0;32m   1117\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m   1118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cum_rewards, actor_losses, critic_losses, total_losses = train_PPO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.distributions.MultivariateNormal(torch.zeros(3), torch.randn(9).reshape(3,3).exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env):\n",
    "    done = False\n",
    "    time.sleep(1.0)\n",
    "    next_state = env_obs_to_tensor(env.reset()[0]).to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    while not done:\n",
    "        action, logprob = model.policy.sample_action_with_logprobs(next_state.to(device).unsqueeze(0))\n",
    "        #action = model.policy.mean_only(next_state.to(device).unsqueeze(0))\n",
    "        clamped_action = np.clip((action.detach().cpu().numpy())[0], -1,1)\n",
    "        next_state, reward, done, truncated, info = env.step(clamped_action)\n",
    "        next_state = env_obs_to_tensor(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent()\n",
    "agent.load_state_dict(torch.load('Y206.65RewardRacer155Update_2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 13.214103900001646\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 3.6319978999999876\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n",
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 6.322334800001045\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n",
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 0.051987200507937814\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evaluate_model(agent, env)\n",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, env)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m#action = model.policy.mean_only(next_state.to(device).unsqueeze(0))\u001b[39;00m\n\u001b[0;32m     10\u001b[0m clamped_action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip((action\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m next_state, reward, done, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clamped_action)\n\u001b[0;32m     12\u001b[0m next_state \u001b[39m=\u001b[39m env_obs_to_tensor(next_state)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\core.py:408\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\n\u001b[0;32m    405\u001b[0m     \u001b[39mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    406\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[0;32m    407\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tmrl\\wrappers.py:33\u001b[0m, in \u001b[0;36mFloat64ToFloat32.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m---> 33\u001b[0m     s, r, d, t, info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m s, r, d, t, info\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\core.py:469\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\n\u001b[0;32m    466\u001b[0m     \u001b[39mself\u001b[39m, action: ActType\n\u001b[0;32m    467\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[0;32m    468\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:545\u001b[0m, in \u001b[0;36mRealTimeEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbenchmark:\n\u001b[0;32m    544\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbench\u001b[39m.\u001b[39mstart_step_time()\n\u001b[1;32m--> 545\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_join_thread()\n\u001b[0;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    547\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_buf\u001b[39m.\u001b[39mappend(action)  \u001b[39m# the action is always appended to the buffer\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:389\u001b[0m, in \u001b[0;36mRealTimeEnv._join_thread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"This is called at the beginning of every user-side API functions (step(), reset()...) for thread safety.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[39mThis ensures that the previous time-step is completed when starting a new one\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39masync_threading:\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_at_thread\u001b[39m.\u001b[39;49mjoin()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[0;32m   1097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1116\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[0;32m   1117\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m   1118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_model(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
